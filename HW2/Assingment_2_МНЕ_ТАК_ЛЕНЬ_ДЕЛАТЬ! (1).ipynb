{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SA6O5PxLTRXQ"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Instructions\n",
        "- Your submission should be the `.ipynb` file with your name,\n",
        "  like `YusufMesbah.ipynb`. it should include the answers to the questions in\n",
        "  markdown cells.\n",
        "- You are expected to follow the best practices for code writing and model\n",
        "training. Poor coding style will be penalized.\n",
        "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
        "Plagiarism in the code will result in failing. If you use code from the\n",
        "internet, cite it.\n",
        "- If the instructions seem vague, use common sense."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: ANN (30%)\n",
        "For this task, you are required to build a fully connect feed-forward ANN model\n",
        "for a multi-label regression problem.\n",
        "\n",
        "For the given data, you need do proper data preprocessing, design the ANN model,\n",
        "then fine-tune your model architecture (number of layers, number of neurons,\n",
        "activation function, learning rate, momentum, regularization).\n",
        "\n",
        "For evaluating your model, do $80/20$ train test split.\n",
        "\n",
        "### Data\n",
        "You will be working with the data in `Task 1.csv` for predicting students'\n",
        "scores in 3 different exams: math, reading and writing. The columns include:\n",
        " - gender\n",
        " - race\n",
        " - parental level of education\n",
        " - lunch meal plan at school\n",
        " - whether the student undertook the test preparation course"
      ],
      "metadata": {
        "collapsed": false,
        "id": "xgaeQQ7RTRXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Preprocessing and encoding"
      ],
      "metadata": {
        "id": "txOb7k8qdciD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('Task 1.csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "Y5NXfG4edbs0",
        "outputId": "bdd72005-6791-4cfd-a103-9985a4f53797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   gender race/ethnicity parental level of education         lunch  \\\n",
              "0    male        group A                 high school      standard   \n",
              "1  female        group D            some high school  free/reduced   \n",
              "2    male        group E                some college  free/reduced   \n",
              "3    male        group B                 high school      standard   \n",
              "4    male        group E          associate's degree      standard   \n",
              "\n",
              "  test preparation course  math score  reading score  writing score  \n",
              "0               completed          67             67             63  \n",
              "1                    none          40             59             55  \n",
              "2                    none          59             60             50  \n",
              "3                    none          77             78             68  \n",
              "4               completed          78             73             68  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2158c946-7fac-4593-bf1d-db63c6ab6bcf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>race/ethnicity</th>\n",
              "      <th>parental level of education</th>\n",
              "      <th>lunch</th>\n",
              "      <th>test preparation course</th>\n",
              "      <th>math score</th>\n",
              "      <th>reading score</th>\n",
              "      <th>writing score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>group A</td>\n",
              "      <td>high school</td>\n",
              "      <td>standard</td>\n",
              "      <td>completed</td>\n",
              "      <td>67</td>\n",
              "      <td>67</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>group D</td>\n",
              "      <td>some high school</td>\n",
              "      <td>free/reduced</td>\n",
              "      <td>none</td>\n",
              "      <td>40</td>\n",
              "      <td>59</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>group E</td>\n",
              "      <td>some college</td>\n",
              "      <td>free/reduced</td>\n",
              "      <td>none</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>group B</td>\n",
              "      <td>high school</td>\n",
              "      <td>standard</td>\n",
              "      <td>none</td>\n",
              "      <td>77</td>\n",
              "      <td>78</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>male</td>\n",
              "      <td>group E</td>\n",
              "      <td>associate's degree</td>\n",
              "      <td>standard</td>\n",
              "      <td>completed</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2158c946-7fac-4593-bf1d-db63c6ab6bcf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2158c946-7fac-4593-bf1d-db63c6ab6bcf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2158c946-7fac-4593-bf1d-db63c6ab6bcf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   math score  reading score  writing score  race/ethnicity_group A  \\\n",
              "0          67             67             63                       1   \n",
              "1          40             59             55                       0   \n",
              "2          59             60             50                       0   \n",
              "3          77             78             68                       0   \n",
              "4          78             73             68                       0   \n",
              "\n",
              "   race/ethnicity_group B  race/ethnicity_group C  race/ethnicity_group D  \\\n",
              "0                       0                       0                       0   \n",
              "1                       0                       0                       1   \n",
              "2                       0                       0                       0   \n",
              "3                       1                       0                       0   \n",
              "4                       0                       0                       0   \n",
              "\n",
              "   race/ethnicity_group E  parental level of education_associate's degree  \\\n",
              "0                       0                                               0   \n",
              "1                       0                                               0   \n",
              "2                       1                                               0   \n",
              "3                       0                                               0   \n",
              "4                       1                                               1   \n",
              "\n",
              "   parental level of education_bachelor's degree  \\\n",
              "0                                              0   \n",
              "1                                              0   \n",
              "2                                              0   \n",
              "3                                              0   \n",
              "4                                              0   \n",
              "\n",
              "   parental level of education_high school  \\\n",
              "0                                        1   \n",
              "1                                        0   \n",
              "2                                        0   \n",
              "3                                        1   \n",
              "4                                        0   \n",
              "\n",
              "   parental level of education_master's degree  \\\n",
              "0                                            0   \n",
              "1                                            0   \n",
              "2                                            0   \n",
              "3                                            0   \n",
              "4                                            0   \n",
              "\n",
              "   parental level of education_some college  \\\n",
              "0                                         0   \n",
              "1                                         0   \n",
              "2                                         1   \n",
              "3                                         0   \n",
              "4                                         0   \n",
              "\n",
              "   parental level of education_some high school  gender_encoded  \\\n",
              "0                                             0               1   \n",
              "1                                             1               0   \n",
              "2                                             0               1   \n",
              "3                                             0               1   \n",
              "4                                             0               1   \n",
              "\n",
              "   test preparation course_encoded  lunch_encoded  \n",
              "0                                0              1  \n",
              "1                                1              0  \n",
              "2                                1              0  \n",
              "3                                1              1  \n",
              "4                                0              1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27c4aa5f-3ede-4387-a078-e45da72bdf3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>math score</th>\n",
              "      <th>reading score</th>\n",
              "      <th>writing score</th>\n",
              "      <th>race/ethnicity_group A</th>\n",
              "      <th>race/ethnicity_group B</th>\n",
              "      <th>race/ethnicity_group C</th>\n",
              "      <th>race/ethnicity_group D</th>\n",
              "      <th>race/ethnicity_group E</th>\n",
              "      <th>parental level of education_associate's degree</th>\n",
              "      <th>parental level of education_bachelor's degree</th>\n",
              "      <th>parental level of education_high school</th>\n",
              "      <th>parental level of education_master's degree</th>\n",
              "      <th>parental level of education_some college</th>\n",
              "      <th>parental level of education_some high school</th>\n",
              "      <th>gender_encoded</th>\n",
              "      <th>test preparation course_encoded</th>\n",
              "      <th>lunch_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67</td>\n",
              "      <td>67</td>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>59</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>77</td>\n",
              "      <td>78</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27c4aa5f-3ede-4387-a078-e45da72bdf3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27c4aa5f-3ede-4387-a078-e45da72bdf3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27c4aa5f-3ede-4387-a078-e45da72bdf3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def encode_features(ds, features_to_encode):\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "  encoder.fit(ds[features_to_encode])\n",
        "\n",
        "  new_feats = encoder.transform(ds[features_to_encode])\n",
        "  # create dataset from encoded features with named columns\n",
        "  new_cols = pd.DataFrame(new_feats, dtype=int, columns=encoder.get_feature_names(features_to_encode))\n",
        "  new_ds = pd.concat([ds, new_cols], axis=1)    \n",
        "  new_ds.drop(features_to_encode, axis=1, inplace=True)\n",
        "  return new_ds\n",
        "\n",
        "def encode_ordinal_features(ds, features_to_encode):\n",
        "  encoder = OrdinalEncoder()\n",
        "  encoder.fit(ds[features_to_encode])\n",
        "\n",
        "  new_feats = encoder.transform(ds[features_to_encode])\n",
        "  # create dataset from encoded features with named columns\n",
        "  new_cols = pd.DataFrame(new_feats, dtype=int, columns=list(map(lambda x: x + \"_encoded\", features_to_encode)))\n",
        "  new_ds = pd.concat([ds, new_cols], axis=1)    \n",
        "  new_ds.drop(features_to_encode, axis=1, inplace=True)\n",
        "  return new_ds\n",
        "\n",
        "\n",
        "def scale_data(x_train, x_test):\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x_train)\n",
        "  \n",
        "  x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
        "  x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
        "  return (x_train, x_test, scaler)\n",
        "\n",
        "df = encode_features(df, [\"race/ethnicity\", \"parental level of education\"])\n",
        "df = encode_ordinal_features(df, [\"gender\", \"test preparation course\", \"lunch\"])\n",
        "df.head(5)\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "e-ztoqcXTRXU",
        "outputId": "7452e697-7d52-40c8-e4a0-bc52d5aeb757"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data"
      ],
      "metadata": {
        "id": "lDBQ76hXuXe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = df.iloc[:,3:]\n",
        "df_y  = df.iloc[:,:3] / 100\n",
        "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(\n",
        "      df_x, df_y, test_size = 0.2, random_state=28)\n",
        "\n",
        "df_x_train, df_x_test, _ = scale_data(df_x_train, df_x_test)"
      ],
      "metadata": {
        "id": "QzxSdZYauW6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating pytorch dataset\n"
      ],
      "metadata": {
        "id": "8iT8TzOZqSEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustumData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.y = torch.tensor(y).float()\n",
        "        self.X = torch.tensor(X).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx, :], self.y[idx]\n",
        "\n",
        "\n",
        "train_dataset = CustumData(df_x_train.values, df_y_train.values)\n",
        "test_dataset = CustumData(df_x_test.values, df_y_test.values) \n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, 128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, 128, shuffle=False)\n",
        "\n",
        "data, label = next(iter(train_dataloader))\n",
        "label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1nImxAnqU4Y",
        "outputId": "45494b7e-cf12-450e-bae5-840f5d092876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a model:"
      ],
      "metadata": {
        "id": "99Eg3oNB6aOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from time import time \n",
        "\n",
        "class ANN_model(nn.Module):\n",
        "    def __init__(self, layers_and_neurons = [64, 64], activation_function = F.leaky_relu):\n",
        "        super(ANN_model, self).__init__()\n",
        "        self.input_layer = nn.Linear(14, layers_and_neurons[0])\n",
        "        for i in range(0, len(layers_and_neurons) - 1):\n",
        "          self.layers.append(nn.Linear(layers_and_neurons[i], layers_and_neurons[i + 1]))\n",
        "        self.output_layer = nn.Linear(layers_and_neurons[-1], 3)\n",
        "        self.activation_function = activation_function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.activation_function(self.input_layer(x))\n",
        "        for i in range(0, len(self.layers)):\n",
        "          x = self.activation_function(self.layers[i](x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "import operator\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "class EarlyStopping():\n",
        "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.prev_metric = np.inf if mode == 'min' else -np.inf\n",
        "\n",
        "        self.operation = operator.gt if mode == 'min' else operator.lt\n",
        "\n",
        "    def __call__(self, metric):\n",
        "        delta = (metric - self.prev_metric)\n",
        "\n",
        "        if self.operation(delta, self.min_delta):\n",
        "            self.counter +=1\n",
        "        else:\n",
        "            self.counter = 0\n",
        "            self.prev_metric = metric\n",
        "\n",
        "        if self.counter >= self.tolerance:\n",
        "            self.early_stop = True\n",
        "        return self.early_stop\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    log_interval = 10\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        \n",
        "        #print(f'Target: {target}')\n",
        "        #print(f'Output: {output}')\n",
        "\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f'\\rTrain Epoch {epoch}:',\n",
        "                  f'[{batch_idx * len(data)}/{len(train_loader.dataset)}',\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]',\n",
        "                  f'\\tLoss: {loss.item():.6f}',\n",
        "                  end='')\n",
        "         \n",
        "    print(f'\\rTrain Epoch: {epoch} Average Loss: {epoch_loss/len(train_loader.dataset):.6f}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss\n",
        "\n",
        "def test( model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    mse_loss = 0\n",
        "    mae_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            mse_loss += F.mse_loss(output, target, reduction='sum').item()\n",
        "            test_loss += F.l1_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            #print(f'Target: {target}')\n",
        "            #print(f'Output: {output}')\n",
        "\n",
        "    print(f'\\rTest set: Average loss: {test_loss/len(test_loader.dataset):.4f}')\n",
        "    \n",
        "    return test_loss, mse_loss\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "a3HLPOb-6Z8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import copy\n",
        "\n",
        "def check_model(model, device, test_dataloader, train_dataloader):\n",
        "  writer = SummaryWriter(log_dir='runs/model')\n",
        "\n",
        "  epochs = 100\n",
        "  lr = 0.1  # will start with big LR to see the lr_scheduler work\n",
        "  momentum = 0.5\n",
        "  log_interval = 10\n",
        "\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=3, verbose=True)\n",
        "  early_stopping = EarlyStopping(tolerance=7, mode='min')\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_loss, best_mse, best_mae = 10000, 120, 200\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss = train(model, device, train_dataloader, criterion, optimizer, epoch)\n",
        "      scheduler.step(train_loss)\n",
        "\n",
        "      test_loss, test_mse = test(model, device, test_dataloader, criterion)\n",
        "      if early_stopping(test_loss):\n",
        "        print('\\nEarly stopping\\n')\n",
        "        break\n",
        "\n",
        "      writer.add_scalars('Loss',\n",
        "                          {\n",
        "                              'train': train_loss,\n",
        "                              'test': test_loss\n",
        "                          },\n",
        "                          epoch)\n",
        "      if test_loss < best_loss:\n",
        "          best_loss = test_loss\n",
        "          best_mse = test_mse\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  writer.close()\n",
        "  # we can also save every X epochs, so if our machine crashes we don't lose our progress\n",
        "  torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  torch.save(model.state_dict(), \"best_model.pt\")\n",
        "  return best_loss, best_mse\n",
        "\n",
        "#check_model(ANN_model(), test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "id": "WR1BYG5JPue7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning model"
      ],
      "metadata": {
        "id": "NcOq1F4ritS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# checking amount of layers\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "three_layer_model = ANN_model().to(device)\n",
        "print(three_layer_model)\n",
        "three_layer_loss, three_layer_mse = check_model(three_layer_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuGgozH7ixKQ",
        "outputId": "45d452e4-f9c3-4338-f861-774e12f95874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN_model(\n",
            "  (input_layer): Linear(in_features=14, out_features=64, bias=True)\n",
            "  (output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "\rTrain Epoch 1: [0/800 (0%)] \tLoss: 0.672741\rTrain Epoch: 1 Average Loss: 0.003356, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.9576\n",
            "\rTrain Epoch 2: [0/800 (0%)] \tLoss: 0.312343\rTrain Epoch: 2 Average Loss: 0.002116, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.6016\n",
            "\rTrain Epoch 3: [0/800 (0%)] \tLoss: 0.214837\rTrain Epoch: 3 Average Loss: 0.001449, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.4637\n",
            "\rTrain Epoch 4: [0/800 (0%)] \tLoss: 0.155134\rTrain Epoch: 4 Average Loss: 0.001207, elapsed time:0.02s\n",
            "\rTest set: Average loss: 0.4327\n",
            "\rTrain Epoch 5: [0/800 (0%)] \tLoss: 0.147981\rTrain Epoch: 5 Average Loss: 0.001094, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3650\n",
            "\rTrain Epoch 6: [0/800 (0%)] \tLoss: 0.130772\rTrain Epoch: 6 Average Loss: 0.001034, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3501\n",
            "\rTrain Epoch 7: [0/800 (0%)] \tLoss: 0.116953\rTrain Epoch: 7 Average Loss: 0.001002, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3378\n",
            "\rTrain Epoch 8: [0/800 (0%)] \tLoss: 0.112610\rTrain Epoch: 8 Average Loss: 0.000956, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3228\n",
            "\rTrain Epoch 9: [0/800 (0%)] \tLoss: 0.118710\rTrain Epoch: 9 Average Loss: 0.000940, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3213\n",
            "\rTrain Epoch 10: [0/800 (0%)] \tLoss: 0.114313\rTrain Epoch: 10 Average Loss: 0.000932, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3085\n",
            "\rTrain Epoch 11: [0/800 (0%)] \tLoss: 0.112191\rTrain Epoch: 11 Average Loss: 0.000926, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3206\n",
            "Train Epoch: 12 Average Loss: 0.000899, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3076\n",
            "Train Epoch: 13 Average Loss: 0.000918, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3162\n",
            "Train Epoch: 14 Average Loss: 0.000901, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3021\n",
            "Train Epoch: 15 Average Loss: 0.000902, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3050\n",
            "Train Epoch: 16 Average Loss: 0.000890, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3043\n",
            "Train Epoch: 17 Average Loss: 0.000899, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3110\n",
            "Train Epoch: 18 Average Loss: 0.000889, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3018\n",
            "Train Epoch: 19 Average Loss: 0.000906, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3022\n",
            "Train Epoch: 20 Average Loss: 0.000873, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3019\n",
            "Train Epoch: 21 Average Loss: 0.000890, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3150\n",
            "Train Epoch: 22 Average Loss: 0.000892, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3042\n",
            "Train Epoch: 23 Average Loss: 0.000881, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3162\n",
            "Train Epoch: 24 Average Loss: 0.000855, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3019\n",
            "Train Epoch: 25 Average Loss: 0.000876, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3057\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fifty_layer_model = ANN_model(layers_and_neurons = [random.randrange(512) for i in range(50)]).to(device)\n",
        "fifty_layer_loss, fifty_layer_mse = check_model(fifty_layer_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnA4Iouilmwr",
        "outputId": "b758703c-8f48-4884-8973-8c446bca72f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.003566, elapsed time:0.38s\n",
            "Test set: Average loss: 0.8725\n",
            "Train Epoch: 2 Average Loss: 0.002512, elapsed time:0.37s\n",
            "Test set: Average loss: 0.4684\n",
            "Train Epoch: 3 Average Loss: 0.001635, elapsed time:0.39s\n",
            "Test set: Average loss: 0.4179\n",
            "Train Epoch: 4 Average Loss: 0.001321, elapsed time:0.37s\n",
            "Test set: Average loss: 0.4298\n",
            "Train Epoch: 5 Average Loss: 0.001230, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3971\n",
            "Train Epoch: 6 Average Loss: 0.001123, elapsed time:0.38s\n",
            "Test set: Average loss: 0.3877\n",
            "Train Epoch: 7 Average Loss: 0.001081, elapsed time:0.40s\n",
            "Test set: Average loss: 0.3808\n",
            "Train Epoch: 8 Average Loss: 0.001096, elapsed time:0.38s\n",
            "Test set: Average loss: 0.3801\n",
            "Train Epoch: 9 Average Loss: 0.001061, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3784\n",
            "Train Epoch: 10 Average Loss: 0.001062, elapsed time:0.37s\n",
            "Test set: Average loss: 0.3790\n",
            "Train Epoch: 11 Average Loss: 0.001078, elapsed time:0.37s\n",
            "Test set: Average loss: 0.3776\n",
            "Train Epoch: 12 Average Loss: 0.001078, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3820\n",
            "Train Epoch: 13 Average Loss: 0.001107, elapsed time:0.36s\n",
            "Epoch 00013: reducing learning rate of group 0 to 3.0000e-02.\n",
            "Test set: Average loss: 0.3786\n",
            "Train Epoch: 14 Average Loss: 0.001086, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3782\n",
            "Train Epoch: 15 Average Loss: 0.001087, elapsed time:0.37s\n",
            "Test set: Average loss: 0.3852\n",
            "Train Epoch: 16 Average Loss: 0.001082, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3786\n",
            "Train Epoch: 17 Average Loss: 0.001073, elapsed time:0.36s\n",
            "Epoch 00017: reducing learning rate of group 0 to 9.0000e-03.\n",
            "Test set: Average loss: 0.3778\n",
            "Train Epoch: 18 Average Loss: 0.001077, elapsed time:0.37s\n",
            "Test set: Average loss: 0.3786\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thousand_layer_model = ANN_model(layers_and_neurons = [random.randrange(512) for i in range(50)]).to(device)\n",
        "thousand_layer_loss, thousand_layer_mse = check_model(thousand_layer_model, device, test_dataloader, train_dataloader)\n",
        "\n",
        "print(\"\\n\\n\\n\\n\")\n",
        "print(f'\\n Three layer model L1 loss: {three_layer_loss}, mse: {three_layer_mse}',\n",
        "          f'\\n Fifty layer model L1 loss: {fifty_layer_loss}, mse: {fifty_layer_mse},', \n",
        "          f'\\n Thousamd layer model L1 loss: {thousand_layer_loss}, mse: {thousand_layer_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IvvBlSxlrho",
        "outputId": "b0a000b4-dfc9-4eb2-a804-412d333b0381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain Epoch 1: [0/800 (0%)] \tLoss: 0.632802"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain Epoch: 1 Average Loss: 0.003717, elapsed time:0.35s\n",
            "Test set: Average loss: 0.4519\n",
            "Train Epoch: 2 Average Loss: 0.002317, elapsed time:0.36s\n",
            "Test set: Average loss: 0.5311\n",
            "Train Epoch: 3 Average Loss: 0.001519, elapsed time:0.36s\n",
            "Test set: Average loss: 0.5420\n",
            "Train Epoch: 4 Average Loss: 0.001357, elapsed time:0.35s\n",
            "Test set: Average loss: 0.4599\n",
            "Train Epoch: 5 Average Loss: 0.001230, elapsed time:0.35s\n",
            "Test set: Average loss: 0.4408\n",
            "Train Epoch: 6 Average Loss: 0.001160, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3942\n",
            "Train Epoch: 7 Average Loss: 0.001138, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3923\n",
            "Train Epoch: 8 Average Loss: 0.001101, elapsed time:0.35s\n",
            "Test set: Average loss: 0.3778\n",
            "Train Epoch: 9 Average Loss: 0.001086, elapsed time:0.35s\n",
            "Test set: Average loss: 0.3786\n",
            "Train Epoch: 10 Average Loss: 0.001105, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3787\n",
            "Train Epoch: 11 Average Loss: 0.001077, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3784\n",
            "Train Epoch: 12 Average Loss: 0.001070, elapsed time:0.34s\n",
            "Test set: Average loss: 0.3783\n",
            "Train Epoch: 13 Average Loss: 0.001078, elapsed time:0.35s\n",
            "Test set: Average loss: 0.3782\n",
            "Train Epoch: 14 Average Loss: 0.001059, elapsed time:0.36s\n",
            "Test set: Average loss: 0.3788\n",
            "Train Epoch: 15 Average Loss: 0.001068, elapsed time:0.35s\n",
            "Test set: Average loss: 0.3806\n",
            "\n",
            "Early stopping\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Three layer model L1 loss: 60.350765228271484, mse: 9.533671379089355 \n",
            " Fifty layer model L1 loss: 75.51679420471191, mse: 14.399005889892578, \n",
            " Thousamd layer model L1 loss: 75.56796646118164, mse: 14.671992301940918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ten_neuron_model = ANN_model(layers_and_neurons=[20, 64, 14]).to(device)\n",
        "ten_neuron_loss, ten_neuron_mse = check_model(ten_neuron_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieeWStpanwOV",
        "outputId": "3eb158b7-caae-4d02-d04f-5564224fa063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain Epoch 1: [0/800 (0%)] \tLoss: 0.612709\rTrain Epoch: 1 Average Loss: 0.003457, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.5576\n",
            "\rTrain Epoch 2: [0/800 (0%)] \tLoss: 0.179423\rTrain Epoch: 2 Average Loss: 0.002117, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.5404\n",
            "\rTrain Epoch 3: [0/800 (0%)] \tLoss: 0.180558\rTrain Epoch: 3 Average Loss: 0.001578, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.4867\n",
            "\rTrain Epoch 4: [0/800 (0%)] \tLoss: 0.159159\rTrain Epoch: 4 Average Loss: 0.001339, elapsed time:0.02s\n",
            "\rTest set: Average loss: 0.4177\n",
            "\rTrain Epoch 5: [0/800 (0%)] \tLoss: 0.136386\rTrain Epoch: 5 Average Loss: 0.001198, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.4106\n",
            "\rTrain Epoch 6: [0/800 (0%)] \tLoss: 0.134681\rTrain Epoch: 6 Average Loss: 0.001122, elapsed time:0.01s\n",
            "\rTest set: Average loss: 0.3846\n",
            "\rTrain Epoch 7: [0/800 (0%)] \tLoss: 0.127633\rTrain Epoch: 7 Average Loss: 0.001104, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3865\n",
            "Train Epoch: 8 Average Loss: 0.001087, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3731\n",
            "Train Epoch: 9 Average Loss: 0.001092, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3754\n",
            "Train Epoch: 10 Average Loss: 0.001048, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3709\n",
            "Train Epoch: 11 Average Loss: 0.001074, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3719\n",
            "Train Epoch: 12 Average Loss: 0.001030, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3681\n",
            "Train Epoch: 13 Average Loss: 0.001037, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3685\n",
            "Train Epoch: 14 Average Loss: 0.001075, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3647\n",
            "Train Epoch: 15 Average Loss: 0.001045, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3665\n",
            "Train Epoch: 16 Average Loss: 0.001044, elapsed time:0.01s\n",
            "Epoch 00016: reducing learning rate of group 0 to 3.0000e-02.\n",
            "Test set: Average loss: 0.3607\n",
            "Train Epoch: 17 Average Loss: 0.001019, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3610\n",
            "Train Epoch: 18 Average Loss: 0.001031, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3587\n",
            "Train Epoch: 19 Average Loss: 0.001011, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3581\n",
            "Train Epoch: 20 Average Loss: 0.001037, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3577\n",
            "Train Epoch: 21 Average Loss: 0.001006, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3567\n",
            "Train Epoch: 22 Average Loss: 0.001019, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3552\n",
            "Train Epoch: 23 Average Loss: 0.000998, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3545\n",
            "Train Epoch: 24 Average Loss: 0.001009, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3547\n",
            "Train Epoch: 25 Average Loss: 0.001012, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3520\n",
            "Train Epoch: 26 Average Loss: 0.001015, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3517\n",
            "Train Epoch: 27 Average Loss: 0.000988, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3511\n",
            "Train Epoch: 28 Average Loss: 0.000997, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3502\n",
            "Train Epoch: 29 Average Loss: 0.000989, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3483\n",
            "Train Epoch: 30 Average Loss: 0.001003, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3465\n",
            "Train Epoch: 31 Average Loss: 0.000996, elapsed time:0.01s\n",
            "Epoch 00031: reducing learning rate of group 0 to 9.0000e-03.\n",
            "Test set: Average loss: 0.3453\n",
            "Train Epoch: 32 Average Loss: 0.000992, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3450\n",
            "Train Epoch: 33 Average Loss: 0.000985, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3443\n",
            "Train Epoch: 34 Average Loss: 0.000999, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3441\n",
            "Train Epoch: 35 Average Loss: 0.000972, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3435\n",
            "Train Epoch: 36 Average Loss: 0.000975, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3433\n",
            "Train Epoch: 37 Average Loss: 0.000995, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3430\n",
            "Train Epoch: 38 Average Loss: 0.000981, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3426\n",
            "Train Epoch: 39 Average Loss: 0.000976, elapsed time:0.02s\n",
            "Epoch 00039: reducing learning rate of group 0 to 2.7000e-03.\n",
            "Test set: Average loss: 0.3426\n",
            "Train Epoch: 40 Average Loss: 0.000983, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3424\n",
            "Train Epoch: 41 Average Loss: 0.000978, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3424\n",
            "Train Epoch: 42 Average Loss: 0.000980, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3423\n",
            "Train Epoch: 43 Average Loss: 0.000983, elapsed time:0.01s\n",
            "Epoch 00043: reducing learning rate of group 0 to 8.1000e-04.\n",
            "Test set: Average loss: 0.3420\n",
            "Train Epoch: 44 Average Loss: 0.000996, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3420\n",
            "Train Epoch: 45 Average Loss: 0.000992, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3420\n",
            "Train Epoch: 46 Average Loss: 0.000983, elapsed time:0.02s\n",
            "Test set: Average loss: 0.3421\n",
            "Train Epoch: 47 Average Loss: 0.000993, elapsed time:0.01s\n",
            "Epoch 00047: reducing learning rate of group 0 to 2.4300e-04.\n",
            "Test set: Average loss: 0.3421\n",
            "Train Epoch: 48 Average Loss: 0.000985, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3421\n",
            "Train Epoch: 49 Average Loss: 0.000963, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3421\n",
            "Train Epoch: 50 Average Loss: 0.000966, elapsed time:0.01s\n",
            "Test set: Average loss: 0.3421\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hundred_neuron_model = ANN_model(layers_and_neurons=[128, 512, 256]).to(device)\n",
        "hundred_neuron_loss, hundred_neuron_mse = check_model(hundred_neuron_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Eq2dqCqOos",
        "outputId": "24d6e146-db7f-499c-8f02-1c1ea1c7dda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain Epoch 1: [0/800 (0%)] \tLoss: 0.678451\rTrain Epoch: 1 Average Loss: 0.003512, elapsed time:0.03s\n",
            "Test set: Average loss: 0.7587\n",
            "Train Epoch: 2 Average Loss: 0.002207, elapsed time:0.03s\n",
            "Test set: Average loss: 0.4379\n",
            "Train Epoch: 3 Average Loss: 0.001435, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3873\n",
            "Train Epoch: 4 Average Loss: 0.001309, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3909\n",
            "Train Epoch: 5 Average Loss: 0.001200, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3625\n",
            "Train Epoch: 6 Average Loss: 0.001115, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3689\n",
            "Train Epoch: 7 Average Loss: 0.001051, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3509\n",
            "Train Epoch: 8 Average Loss: 0.001013, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3501\n",
            "Train Epoch: 9 Average Loss: 0.000985, elapsed time:0.04s\n",
            "Test set: Average loss: 0.3344\n",
            "Train Epoch: 10 Average Loss: 0.000989, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3316\n",
            "Train Epoch: 11 Average Loss: 0.000952, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3227\n",
            "Train Epoch: 12 Average Loss: 0.000940, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3156\n",
            "Train Epoch: 13 Average Loss: 0.000929, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3206\n",
            "Train Epoch: 14 Average Loss: 0.000919, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3099\n",
            "Train Epoch: 15 Average Loss: 0.000911, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3062\n",
            "Train Epoch: 16 Average Loss: 0.000912, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3059\n",
            "Train Epoch: 17 Average Loss: 0.000900, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3033\n",
            "Train Epoch: 18 Average Loss: 0.000895, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3049\n",
            "Train Epoch: 19 Average Loss: 0.000913, elapsed time:0.04s\n",
            "Test set: Average loss: 0.3037\n",
            "Train Epoch: 20 Average Loss: 0.000901, elapsed time:0.04s\n",
            "Test set: Average loss: 0.3042\n",
            "Train Epoch: 21 Average Loss: 0.000907, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3037\n",
            "Train Epoch: 22 Average Loss: 0.000884, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3059\n",
            "Train Epoch: 23 Average Loss: 0.000892, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3129\n",
            "Train Epoch: 24 Average Loss: 0.000889, elapsed time:0.03s\n",
            "Test set: Average loss: 0.3040\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thousand_neuron_model = ANN_model(layers_and_neurons=[2048, 5000, 1024]).to(device)\n",
        "thousand_neuron_loss, thousand_neuron_mse = check_model(thousand_neuron_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf0uL7xSqaxP",
        "outputId": "d0446de2-e726-4494-bc7b-2c50e279ef87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.003235, elapsed time:1.29s\n",
            "Test set: Average loss: 1.1461\n",
            "Train Epoch: 2 Average Loss: 0.002215, elapsed time:1.29s\n",
            "Test set: Average loss: 0.8089\n",
            "Train Epoch: 3 Average Loss: 0.001618, elapsed time:1.28s\n",
            "Test set: Average loss: 0.4994\n",
            "Train Epoch: 4 Average Loss: 0.001324, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3497\n",
            "Train Epoch: 5 Average Loss: 0.001114, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3777\n",
            "Train Epoch: 6 Average Loss: 0.000986, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3309\n",
            "Train Epoch: 7 Average Loss: 0.000956, elapsed time:1.29s\n",
            "Test set: Average loss: 0.3110\n",
            "Train Epoch: 8 Average Loss: 0.000952, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3181\n",
            "Train Epoch: 9 Average Loss: 0.000941, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3083\n",
            "Train Epoch: 10 Average Loss: 0.000916, elapsed time:1.54s\n",
            "Test set: Average loss: 0.3113\n",
            "Train Epoch: 11 Average Loss: 0.000934, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3005\n",
            "Train Epoch: 12 Average Loss: 0.000897, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3363\n",
            "Train Epoch: 13 Average Loss: 0.000966, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3039\n",
            "Train Epoch: 14 Average Loss: 0.000890, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3067\n",
            "Train Epoch: 15 Average Loss: 0.000916, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3188\n",
            "Train Epoch: 16 Average Loss: 0.000884, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3094\n",
            "Train Epoch: 17 Average Loss: 0.000879, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3153\n",
            "Train Epoch: 18 Average Loss: 0.000918, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3064\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\rTen neuron model L1 loss: {ten_neuron_loss}, mse: {ten_neuron_mse}',\n",
        "          f'\\n Hundred neuron model L1 loss: {hundred_neuron_loss}, mse: {hundred_neuron_mse}', \n",
        "          f'\\n Thousamd neuron model L1 loss: {thousand_neuron_loss}, mse: {thousand_neuron_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIwWY5nSqqag",
        "outputId": "051d29aa-2a3c-4f8f-8671-60def109e522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTen neuron model L1 loss: 68.4075927734375, mse: 11.78477168083191 \n",
            " Hundred neuron model L1 loss: 60.65543556213379, mse: 9.514414310455322 \n",
            " Thousamd neuron model L1 loss: 60.099090576171875, mse: 9.323148965835571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu_model = ANN_model(layers_and_neurons=[2048, 5000, 1024], activation_function = F.relu).to(device)\n",
        "relu_loss, relu_mse  = check_model(relu_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQKbjxyU86SD",
        "outputId": "f31e3a32-7970-41c1-8e34-d7e996ae4e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.003435, elapsed time:1.28s\n",
            "Test set: Average loss: 1.2160\n",
            "Train Epoch: 2 Average Loss: 0.002222, elapsed time:1.31s\n",
            "Test set: Average loss: 0.7475\n",
            "Train Epoch: 3 Average Loss: 0.001665, elapsed time:1.30s\n",
            "Test set: Average loss: 0.3753\n",
            "Train Epoch: 4 Average Loss: 0.001244, elapsed time:1.67s\n",
            "Test set: Average loss: 0.3787\n",
            "Train Epoch: 5 Average Loss: 0.001089, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3738\n",
            "Train Epoch: 6 Average Loss: 0.000945, elapsed time:1.80s\n",
            "Test set: Average loss: 0.3126\n",
            "Train Epoch: 7 Average Loss: 0.000943, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3081\n",
            "Train Epoch: 8 Average Loss: 0.000932, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3060\n",
            "Train Epoch: 9 Average Loss: 0.000884, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3165\n",
            "Train Epoch: 10 Average Loss: 0.000906, elapsed time:1.63s\n",
            "Test set: Average loss: 0.3013\n",
            "Train Epoch: 11 Average Loss: 0.000904, elapsed time:1.61s\n",
            "Test set: Average loss: 0.3010\n",
            "Train Epoch: 12 Average Loss: 0.000907, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3017\n",
            "Train Epoch: 13 Average Loss: 0.000896, elapsed time:1.28s\n",
            "Epoch 00013: reducing learning rate of group 0 to 3.0000e-02.\n",
            "Test set: Average loss: 0.3044\n",
            "Train Epoch: 14 Average Loss: 0.000881, elapsed time:1.61s\n",
            "Test set: Average loss: 0.3027\n",
            "Train Epoch: 15 Average Loss: 0.000909, elapsed time:3.78s\n",
            "Test set: Average loss: 0.3056\n",
            "Train Epoch: 16 Average Loss: 0.000894, elapsed time:2.08s\n",
            "Test set: Average loss: 0.3026\n",
            "Train Epoch: 17 Average Loss: 0.000887, elapsed time:1.90s\n",
            "Test set: Average loss: 0.3030\n",
            "Train Epoch: 18 Average Loss: 0.000879, elapsed time:1.29s\n",
            "Test set: Average loss: 0.3029\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu_model = ANN_model(layers_and_neurons=[2048, 5000, 1024], activation_function = F.leaky_relu).to(device)\n",
        "leaky_relu_loss, leaky_relu_mse = check_model(leaky_relu_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oROUziBsAdJY",
        "outputId": "6b8561ac-1b62-47c7-af61-3bb915c941f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.003306, elapsed time:1.27s\n",
            "Test set: Average loss: 1.2253\n",
            "Train Epoch: 2 Average Loss: 0.002320, elapsed time:1.28s\n",
            "Test set: Average loss: 0.7982\n",
            "Train Epoch: 3 Average Loss: 0.001608, elapsed time:1.29s\n",
            "Test set: Average loss: 0.3791\n",
            "Train Epoch: 4 Average Loss: 0.001219, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3824\n",
            "Train Epoch: 5 Average Loss: 0.001106, elapsed time:1.26s\n",
            "Test set: Average loss: 0.3848\n",
            "Train Epoch: 6 Average Loss: 0.001035, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3309\n",
            "Train Epoch: 7 Average Loss: 0.001003, elapsed time:1.26s\n",
            "Test set: Average loss: 0.3186\n",
            "Train Epoch: 8 Average Loss: 0.000971, elapsed time:1.25s\n",
            "Test set: Average loss: 0.3017\n",
            "Train Epoch: 9 Average Loss: 0.000937, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3061\n",
            "Train Epoch: 10 Average Loss: 0.000900, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3581\n",
            "Train Epoch: 11 Average Loss: 0.000974, elapsed time:1.29s\n",
            "Test set: Average loss: 0.3010\n",
            "Train Epoch: 12 Average Loss: 0.000924, elapsed time:1.70s\n",
            "Test set: Average loss: 0.2965\n",
            "Train Epoch: 13 Average Loss: 0.000889, elapsed time:1.27s\n",
            "Test set: Average loss: 0.2956\n",
            "Train Epoch: 14 Average Loss: 0.000885, elapsed time:1.26s\n",
            "Test set: Average loss: 0.2976\n",
            "Train Epoch: 15 Average Loss: 0.000900, elapsed time:1.26s\n",
            "Test set: Average loss: 0.3309\n",
            "Train Epoch: 16 Average Loss: 0.000926, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3003\n",
            "Train Epoch: 17 Average Loss: 0.000877, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3076\n",
            "Train Epoch: 18 Average Loss: 0.000897, elapsed time:1.28s\n",
            "Test set: Average loss: 0.3020\n",
            "Train Epoch: 19 Average Loss: 0.000867, elapsed time:1.27s\n",
            "Test set: Average loss: 0.3031\n",
            "Train Epoch: 20 Average Loss: 0.000904, elapsed time:1.27s\n",
            "Test set: Average loss: 0.2979\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nAverage relu L1 loss: {relu_loss}, mse: {relu_mse}',\n",
        "          f'\\nAverage leaky relu L1 loss: {leaky_relu_loss}, mse: {leaky_relu_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B34lshETBEIM",
        "outputId": "184492ff-3255-4fa7-b294-199653b54ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Average relu L1 loss: 60.19511604309082, mse: 9.347373485565186 Average leaky relu L1 loss: 59.110633850097656, mse: 9.132680416107178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Best_ANN_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Best_ANN_model, self).__init__()\n",
        "        self.input_layer = nn.Linear(14, 2048)\n",
        "        self.hidden1 = nn.Linear(2048, 5000)\n",
        "        self.hidden2 = nn.Linear(5000, 1024)\n",
        "        self.output_layer = nn.Linear(1024, 3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.input_layer(x))\n",
        "        x = F.leaky_relu(self.hidden1(x))\n",
        "        x = F.leaky_relu(self.hidden2(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "final_model = Best_ANN_model().to(device)\n",
        "final_loss, final_mse = check_model(final_model, device, test_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcxfD94cT-fC",
        "outputId": "b8b246b5-c00c-4c52-f19f-f44e591786af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.002808, elapsed time:1.59s\n",
            "Test set: Average loss: 0.5445\n",
            "Train Epoch: 2 Average Loss: 0.001343, elapsed time:1.56s\n",
            "Test set: Average loss: 0.3706\n",
            "Train Epoch: 3 Average Loss: 0.001092, elapsed time:1.56s\n",
            "Test set: Average loss: 0.3651\n",
            "Train Epoch: 4 Average Loss: 0.001106, elapsed time:1.54s\n",
            "Test set: Average loss: 0.3741\n",
            "Train Epoch: 5 Average Loss: 0.001045, elapsed time:1.54s\n",
            "Test set: Average loss: 0.3334\n",
            "Train Epoch: 6 Average Loss: 0.000939, elapsed time:1.52s\n",
            "Test set: Average loss: 0.3261\n",
            "Train Epoch: 7 Average Loss: 0.000968, elapsed time:1.56s\n",
            "Test set: Average loss: 0.3490\n",
            "Train Epoch: 8 Average Loss: 0.000916, elapsed time:1.55s\n",
            "Test set: Average loss: 0.3088\n",
            "Train Epoch: 9 Average Loss: 0.000882, elapsed time:1.52s\n",
            "Test set: Average loss: 0.3083\n",
            "Train Epoch: 10 Average Loss: 0.000869, elapsed time:1.55s\n",
            "Test set: Average loss: 0.3048\n",
            "Train Epoch: 11 Average Loss: 0.000858, elapsed time:1.53s\n",
            "Test set: Average loss: 0.3097\n",
            "Train Epoch: 12 Average Loss: 0.000844, elapsed time:1.54s\n",
            "Test set: Average loss: 0.3119\n",
            "Train Epoch: 13 Average Loss: 0.000851, elapsed time:1.53s\n",
            "Test set: Average loss: 0.3180\n",
            "Train Epoch: 14 Average Loss: 0.000828, elapsed time:1.52s\n",
            "Test set: Average loss: 0.3058\n",
            "Train Epoch: 15 Average Loss: 0.000839, elapsed time:2.24s\n",
            "Test set: Average loss: 0.3096\n",
            "Train Epoch: 16 Average Loss: 0.000845, elapsed time:1.55s\n",
            "Test set: Average loss: 0.3288\n",
            "Train Epoch: 17 Average Loss: 0.000831, elapsed time:1.54s\n",
            "Test set: Average loss: 0.3103\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\Final L1 loss: {relu_loss}, mse: {relu_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD6skG8QWQUf",
        "outputId": "df9b5c1d-4698-4b1d-cc5e-09ae3fbffb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Final L1 loss: 60.19511604309082, mse: 9.347373485565186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. What preprocessing techniques did you use? Why?\n",
        "    - *Answer*\n",
        "2. Describe the fine-tuning process and how you reached your model architecture.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "id": "_BAQrxT2TRXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: CNN (40%)\n",
        "For this task, you will be doing image classification:\n",
        "- First, adapt your best model from Task 1 to work on this task, and\n",
        "fit it on the new data. Then, evaluate its performance.\n",
        "- After that, build a CNN model for image classification.\n",
        "- Compare both models in terms of accuracy, number of parameters and speed of\n",
        "inference (the time the model takes to predict 50 samples).\n",
        "\n",
        "For the given data, you need to do proper data preprocessing and augmentation,\n",
        "data loaders.\n",
        "Then fine-tune your model architecture (number of layers, number of filters,\n",
        "activation function, learning rate, momentum, regularization).\n",
        "\n",
        "### Data\n",
        "You will be working with the data in `triple_mnist.zip` for predicting 3-digit\n",
        "numbers writen in the image. Each image contains 3 digits similar to the\n",
        "following example (whose label is `039`):\n",
        "\n",
        "![example](https://github.com/shaohua0116/MultiDigitMNIST/blob/master/asset/examples/039/0_039.png?raw=true)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ckj2crXtTRXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip triple_mnist.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCoANmRKrT0J",
        "outputId": "123736cd-f0df-403c-d3cc-1e6e470a16f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  triple_mnist.zip\n",
            "replace triple_mnist/test/002/0_002.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.datasets  import ImageFolder\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "\n",
        "class MyImageDataSet(ImageFolder):\n",
        "  def __init__(self, root, transform=None):\n",
        "      super().__init__(root, transform)\n",
        "  \n",
        "  def find_classes(self, directory: str):\n",
        "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "    if not classes:\n",
        "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
        "\n",
        "    class_to_idx = {cls_name: int(cls_name) for i, cls_name in enumerate(classes)}\n",
        "    return classes, class_to_idx \n",
        "\n",
        "train_dataset = MyImageDataSet(root='triple_mnist/train', \n",
        "                             transform=T.Compose([T.ToTensor(), T.Grayscale(), T.RandomRotation(degrees=(-3, 3))]))\n",
        "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=32,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=1)\n",
        "\n",
        "\n",
        "val_dataset = MyImageDataSet(root='triple_mnist/val', \n",
        "                             transform=T.Compose([T.ToTensor(), T.Grayscale()]))\n",
        "val_dataloader = data.DataLoader(dataset=val_dataset,\n",
        "                                  batch_size=32,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=1)\n",
        "\n",
        "\n",
        "test_dataset = MyImageDataSet(root='triple_mnist/test', \n",
        "                             transform=T.Compose([T.ToTensor(), T.Grayscale()]))\n",
        "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
        "                                  batch_size=32,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=1)\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L8BqNpYITRXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "d8fEq3aJA69R",
        "outputId": "4032f40d-3508-4871-e5fb-4df728c09ffd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([32, 1, 84, 84])\n",
            "Labels batch shape: torch.Size([32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from time import time \n",
        "from torch import optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import copy\n",
        "\n",
        "class ANN_img_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN_img_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.adapter = nn.Linear(84*84, 14)\n",
        "        self.input_layer = nn.Linear(14, 2048)\n",
        "        self.hidden1 = nn.Linear(2048, 5000)\n",
        "        self.hidden2 = nn.Linear(5000, 1024)\n",
        "        self.output_1 = nn.Linear(1024, 10)\n",
        "        self.output_2 = nn.Linear(1024, 10)\n",
        "        self.output_3 = nn.Linear(1024, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.adapter(x)\n",
        "        x = F.leaky_relu(self.input_layer(x))\n",
        "        x = F.leaky_relu(self.hidden1(x))\n",
        "        x = F.leaky_relu(self.hidden2(x))\n",
        "        return [self.output_1(x), self.output_2(x), self.output_3(x)]\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "\n",
        "import operator\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "class EarlyStopping():\n",
        "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.prev_metric = np.inf if mode == 'min' else -np.inf\n",
        "\n",
        "        self.operation = operator.gt if mode == 'min' else operator.lt\n",
        "\n",
        "    def __call__(self, metric):\n",
        "        delta = (metric - self.prev_metric)\n",
        "\n",
        "        if self.operation(delta, self.min_delta):\n",
        "            self.counter +=1\n",
        "        else:\n",
        "            self.counter = 0\n",
        "            self.prev_metric = metric\n",
        "\n",
        "        if self.counter >= self.tolerance:\n",
        "            self.early_stop = True\n",
        "        return self.early_stop\n",
        "\n",
        "def customLoss(pred, target):\n",
        "    fir = F.cross_entropy(pred[0], target[0])\n",
        "    sec = F.cross_entropy(pred[1], target[1])\n",
        "    thr = F.cross_entropy(pred[2], target[2])\n",
        "    loss = fir + sec + thr\n",
        "    return loss\n",
        "\n",
        "def img_train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    log_interval = 10\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        target1 = (target // 100).to(device)\n",
        "        target2 = ((target // 10) % 10).to(device) \n",
        "        target3 = (target % 10).to(device)\n",
        "        #print(f'base: {target}')\n",
        "        #print(f'first: {target1}, \\n second: {target2}, \\n third: {target3}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = customLoss(output, [target1, target2, target3])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f'\\rTrain Epoch {epoch}:',\n",
        "                  f'[{batch_idx * len(data)}/{len(train_loader.dataset)}',\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]',\n",
        "                  f'\\tLoss: {loss.item()}',\n",
        "                  end='')\n",
        "              \n",
        "    print(f'\\rTrain Epoch: {epoch} Average Loss: {epoch_loss/len(train_loader.dataset):.6f}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss\n",
        "\n",
        "def img_test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target1 = (target // 100).to(device)\n",
        "            target2 = ((target // 10) % 10).to(device) \n",
        "            target3 = (target % 10).to(device)\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            fir = torch.max(output[0], 1)[1].data.squeeze()\n",
        "            sec = torch.max(output[1], 1)[1].data.squeeze()\n",
        "            thr = torch.max(output[2], 1)[1].data.squeeze()\n",
        "            temp = torch.mul((fir == target1), (sec == target2))\n",
        "            temp = torch.mul(temp, (thr == target3))\n",
        "            correct += temp.sum().item()\n",
        "            total += target1.size(0)\n",
        "\n",
        "            test_loss += customLoss(output, [target1, target2, target3]).item()  # sum up batch loss\n",
        "    \n",
        "    acc = 100. * correct / total\n",
        "    print(f'\\rTest set: Average loss: {test_loss/len(test_loader.dataset):.4f},',\n",
        "          f'Accuracy: {correct}/{len(test_loader.dataset)}', \n",
        "          f'({acc:.0f}%)\\n')\n",
        "    \n",
        "    return test_loss, acc\n",
        "\n",
        "\n",
        "\n",
        "def img_check_model(model, device, test_dataloader, train_dataloader):\n",
        "  writer = SummaryWriter(log_dir='runs/model')\n",
        "  epochs = 15\n",
        "  lr = 5e-2  \n",
        "  momentum = 0.5\n",
        "  log_interval = 10\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=3, verbose=True)\n",
        "  early_stopping = EarlyStopping(tolerance=7, min_delta=5)\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss = img_train(model, device, train_dataloader, criterion, optimizer, epoch)\n",
        "      scheduler.step(train_loss)\n",
        "\n",
        "      test_loss, test_acc = img_test(model, device, test_dataloader, criterion)\n",
        "      if early_stopping(test_loss):\n",
        "          print('\\nEarly stopping\\n')\n",
        "          break\n",
        "\n",
        "      writer.add_scalars('Loss',\n",
        "                          {\n",
        "                              'train': train_loss,\n",
        "                              'test': test_loss\n",
        "                          },\n",
        "                          epoch)\n",
        "      \n",
        "      writer.add_scalars('Accuracy',\n",
        "                          {\n",
        "                              'test': test_acc\n",
        "                          },\n",
        "                          epoch)\n",
        "    \n",
        "      # deep copy the model\n",
        "      if test_acc > best_acc:\n",
        "          best_acc = test_acc\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  writer.close()\n",
        "  torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  torch.save(model.state_dict(), \"best_model.pt\")\n",
        "  return best_acc\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDgtFHb9LmeQ",
        "outputId": "acdaab11-8542-456e-b647-374542fb66e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.194099, elapsed time:68.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:110: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTest set: Average loss: 0.1909, Accuracy: 172/16000 (1%)\n",
            "\n",
            "Train Epoch: 2 Average Loss: 0.184043, elapsed time:67.74s\n",
            "Test set: Average loss: 0.1775, Accuracy: 291/16000 (2%)\n",
            "\n",
            "Train Epoch: 3 Average Loss: 0.162469, elapsed time:67.64s\n",
            "Test set: Average loss: 0.1603, Accuracy: 711/16000 (4%)\n",
            "\n",
            "Train Epoch: 4 Average Loss: 0.154270, elapsed time:67.82s\n",
            "Test set: Average loss: 0.1610, Accuracy: 726/16000 (5%)\n",
            "\n",
            "Train Epoch: 5 Average Loss: 0.152337, elapsed time:67.78s\n",
            "Test set: Average loss: 0.1527, Accuracy: 777/16000 (5%)\n",
            "\n",
            "Train Epoch: 6 Average Loss: 0.151853, elapsed time:68.47s\n",
            "Test set: Average loss: 0.1550, Accuracy: 820/16000 (5%)\n",
            "\n",
            "Train Epoch: 7 Average Loss: 0.151479, elapsed time:67.97s\n",
            "Test set: Average loss: 0.1568, Accuracy: 657/16000 (4%)\n",
            "\n",
            "Train Epoch: 8 Average Loss: 0.152269, elapsed time:74.87s\n",
            "Test set: Average loss: 0.1582, Accuracy: 759/16000 (5%)\n",
            "\n",
            "Train Epoch: 9 Average Loss: 0.152344, elapsed time:69.60s\n",
            "Test set: Average loss: 0.1582, Accuracy: 786/16000 (5%)\n",
            "\n",
            "Train Epoch: 10 Average Loss: 0.152806, elapsed time:67.03s\n",
            "Test set: Average loss: 0.1585, Accuracy: 717/16000 (4%)\n",
            "\n",
            "Train Epoch: 11 Average Loss: 0.153840, elapsed time:73.99s\n",
            "Epoch 00011: reducing learning rate of group 0 to 1.5000e-02.\n",
            "Test set: Average loss: 0.1602, Accuracy: 700/16000 (4%)\n",
            "\n",
            "Train Epoch: 12 Average Loss: 0.131172, elapsed time:76.55s\n",
            "Test set: Average loss: 0.1363, Accuracy: 1338/16000 (8%)\n",
            "\n",
            "Train Epoch: 13 Average Loss: 0.124889, elapsed time:69.61s\n",
            "Test set: Average loss: 0.1385, Accuracy: 1216/16000 (8%)\n",
            "\n",
            "Train Epoch: 14 Average Loss: 0.122086, elapsed time:71.62s\n",
            "Test set: Average loss: 0.1374, Accuracy: 1267/16000 (8%)\n",
            "\n",
            "Train Epoch: 15 Average Loss: 0.119972, elapsed time:73.22s\n",
            "Test set: Average loss: 0.1373, Accuracy: 1304/16000 (8%)\n",
            "\n",
            "Train Epoch: 16 Average Loss: 0.118452, elapsed time:67.21s\n",
            "Test set: Average loss: 0.1394, Accuracy: 1226/16000 (8%)\n",
            "\n",
            "Train Epoch: 17 Average Loss: 0.116879, elapsed time:66.03s\n",
            "Test set: Average loss: 0.1371, Accuracy: 1285/16000 (8%)\n",
            "\n",
            "Train Epoch: 18 Average Loss: 0.115636, elapsed time:70.50s\n",
            "Test set: Average loss: 0.1404, Accuracy: 1173/16000 (7%)\n",
            "\n",
            "Train Epoch: 19 Average Loss: 0.114688, elapsed time:76.79s\n",
            "Test set: Average loss: 0.1422, Accuracy: 1205/16000 (8%)\n",
            "\n",
            "\n",
            "Early stopping\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.3625"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ANN_model = ANN_img_model().to(device)\n",
        "img_check_model(ANN_model, device, val_dataloader, train_dataloader)"
      ],
      "metadata": {
        "id": "Y706yJ1Qe8iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(16, 32, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out1 = nn.Linear(14112, 10)\n",
        "        self.out2 = nn.Linear(14112, 10)\n",
        "        self.out3 = nn.Linear(14112, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return [\n",
        "            self.out1(x),\n",
        "            self.out2(x),\n",
        "            self.out3(x)\n",
        "        ]\n",
        "\n",
        "model = CNN().to(device)\n",
        "img_check_model(model, device, val_dataloader, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YuGM3RO_hVA",
        "outputId": "0e9b319b-87fe-4d16-ddad-5df113a3b468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 Average Loss: 0.055517, elapsed time:64.10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:110: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTest set: Average loss: 0.0328, Accuracy: 11647/16000 (73%)\n",
            "\n",
            "Train Epoch: 2 Average Loss: 0.023351, elapsed time:63.73s\n",
            "Test set: Average loss: 0.0236, Accuracy: 12533/16000 (78%)\n",
            "\n",
            "Train Epoch: 3 Average Loss: 0.018618, elapsed time:63.84s\n",
            "Test set: Average loss: 0.0280, Accuracy: 12767/16000 (80%)\n",
            "\n",
            "Train Epoch: 4 Average Loss: 0.019231, elapsed time:63.97s\n",
            "Test set: Average loss: 0.0363, Accuracy: 12720/16000 (80%)\n",
            "\n",
            "Train Epoch: 5 Average Loss: 0.022872, elapsed time:64.13s\n",
            "Test set: Average loss: 0.0329, Accuracy: 12938/16000 (81%)\n",
            "\n",
            "Train Epoch: 6 Average Loss: 0.024377, elapsed time:63.62s\n",
            "Test set: Average loss: 0.0448, Accuracy: 12067/16000 (75%)\n",
            "\n",
            "Train Epoch: 7 Average Loss: 0.030824, elapsed time:63.48s\n",
            "Epoch 00007: reducing learning rate of group 0 to 1.5000e-02.\n",
            "Test set: Average loss: 0.0571, Accuracy: 11947/16000 (75%)\n",
            "\n",
            "Train Epoch: 8 Average Loss: 0.010195, elapsed time:63.96s\n",
            "Test set: Average loss: 0.0245, Accuracy: 14077/16000 (88%)\n",
            "\n",
            "Train Epoch: 9 Average Loss: 0.005453, elapsed time:64.17s\n",
            "Test set: Average loss: 0.0216, Accuracy: 14288/16000 (89%)\n",
            "\n",
            "Train Epoch: 10 Average Loss: 0.003778, elapsed time:63.86s\n",
            "Test set: Average loss: 0.0206, Accuracy: 14385/16000 (90%)\n",
            "\n",
            "Train Epoch: 11 Average Loss: 0.003212, elapsed time:63.77s\n",
            "Test set: Average loss: 0.0203, Accuracy: 14446/16000 (90%)\n",
            "\n",
            "Train Epoch: 12 Average Loss: 0.002813, elapsed time:63.70s\n",
            "Test set: Average loss: 0.0204, Accuracy: 14496/16000 (91%)\n",
            "\n",
            "Train Epoch: 13 Average Loss: 0.002223, elapsed time:64.05s\n",
            "Test set: Average loss: 0.0207, Accuracy: 14515/16000 (91%)\n",
            "\n",
            "Train Epoch 14: [14720/64000 (23%)] \tLoss: 0.055229343473911285"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = img_test(model, device, test_dataloader, None)\n",
        "print(f'Test dataset loss: {test_loss}, test dataset accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J_P8U1DOerh",
        "outputId": "2b3ef90a-ae5f-46ab-b9e2-fe1b5a7c247c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:110: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTest set: Average loss: 0.0142, Accuracy: 18631/20000 (93%)\n",
            "\n",
            "Test dataset loss: 283.32806559665823, test dataset accuracy: 93.155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. What preprocessing techniques did you use? Why?\n",
        "    - *Answer*\n",
        "2. What data augmentation techniques did you use?\n",
        "    - *Answer*\n",
        "3. Describe the fine-tuning process and how you reached your final CNN model.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TIrSMaNCTRXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Decision Trees and Ensemble Learning (15%)\n",
        "\n",
        "For the `loan_data.csv` data, predict if the bank should give a loan or not.\n",
        "You need to do the following:\n",
        "- Fine-tune a decision tree on the data\n",
        "- Fine-tune a random forest on the data\n",
        "- Compare their performance\n",
        "- Visualize your DT and one of the trees from the RF\n",
        "\n",
        "For evaluating your models, do $80/20$ train test split.\n",
        "\n",
        "### Data\n",
        "- `credit.policy`: Whether the customer meets the credit underwriting criteria.\n",
        "- `purpose`: The purpose of the loan.\n",
        "- `int.rate`: The interest rate of the loan.\n",
        "- `installment`: The monthly installments owed by the borrower if the loan is funded.\n",
        "- `log.annual.inc`: The natural logarithm of the self-reported annual income of the borrower.\n",
        "- `dti`: The debt-to-income ratio of the borrower.\n",
        "- `fico`: The FICO credit score of the borrower.\n",
        "- `days.with.cr.line`: The number of days the borrower has had a credit line.\n",
        "- `revol.bal`: The borrower's revolving balance.\n",
        "- `revol.util`: The borrower's revolving line utilization rate."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LKuHDACrTRXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing:"
      ],
      "metadata": {
        "id": "cYdMM7jhIlYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "df = pd.read_csv('loan_data.csv')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-GZeZNl9TRXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = encode_ordinal_features(df, ['purpose'])\n",
        "y = df['credit.policy']\n",
        "x = df.drop(columns=['credit.policy'])"
      ],
      "metadata": {
        "id": "FGvdGsMNHqEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree:"
      ],
      "metadata": {
        "id": "Ju1tIndSImqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "dt = DecisionTreeRegressor()\n",
        "dt = dt.fit(x_train, y_train)\n",
        "\n",
        "score = dt.score(x_test, y_test)\n",
        "print(\"Score: \", score)"
      ],
      "metadata": {
        "id": "LBqVVjLVIkdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "def plot_tree(clf):\n",
        "    dot_data = tree.export_graphviz(clf, out_file=None,\n",
        "                       feature_names=x.columns.values,\n",
        "                       class_names=y.columns.values,\n",
        "                       filled=True, rounded=True,\n",
        "                       special_characters=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    return graph\n",
        "graph = plot_tree(dt)\n",
        "graph"
      ],
      "metadata": {
        "id": "FOJjhP23Jry6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "emPsBVvOMpeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterions = ['friedman_mse', 'squared_error', 'absolute_error', 'poisson']\n",
        "\n",
        "\n",
        "best_score = 0\n",
        "best_criterion = ''\n",
        "\n",
        "for criterion in criterions:\n",
        "  dt = DecisionTreeRegressor(criterion=criterion)\n",
        "  dt = dt.fit(x_train, y_train)\n",
        "  score = dt.score(x_test, y_test)\n",
        "        \n",
        "  print(f'Criterion: {criterion} \\n Score: {score}')\n",
        "\n",
        "  if score > best_score:\n",
        "      best_score = score\n",
        "      best_criterion = criterion\n",
        "print(f'Best criterion: {criterion} \\n Best score: {score}')"
      ],
      "metadata": {
        "id": "fYqsYlKwNPnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. How did the DT compare to the RF in performance? Why?\n",
        "    - *Answer*\n",
        "2. After fine-tuning, how does the max depth in DT compare to RF? Why?\n",
        "    - *Answer*\n",
        "3. What is ensemble learning? What are its pros and cons?\n",
        "    - *Answer*\n",
        "4. Briefly explain 2 types of boosting methods and 2 types of bagging methods.\n",
        "Which of these categories does RF fall under?\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "id": "D4IWOgD8TRXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Domain Gap (15%)\n",
        "\n",
        "Evaluate your CNN model from task 2 on SVHN data without retraining your model."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "o4N28Or8TRXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Implement task 4"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bJGewsfSTRXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. How did your model perform? Why is it better/worse?\n",
        "    - *Answer*\n",
        "2. What is domain gap in the context of ML?\n",
        "    - *Answer*\n",
        "3. Suggest two ways through which the problem of domain gap can be tackled.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LZnYe2HaTRXc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}